{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Ticket Reply Evaluator\n",
    "**Rauda AI ‚Äî Take-Home Assignment**\n",
    "\n",
    "Eval√∫a respuestas de soporte al cliente usando Llama 3.3 70B v√≠a Groq API.\n",
    "Para cada par (ticket, reply) el modelo devuelve:\n",
    "- `content_score` + `content_explanation`\n",
    "- `format_score` + `format_explanation`\n",
    "\n",
    "**Escala:** 1 (muy malo) ‚Üí 5 (excelente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las dependencias est√°n en requirements.txt\n",
    "# Consulta el README para instrucciones de instalaci√≥n con venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e63d4c",
   "metadata": {},
   "source": [
    "## 2. Imports y configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cargamos la API key desde .env para no escribirla directamente en el c√≥digo.\n",
    "# As√≠ evitamos exponer credenciales si el proyecto se sube a GitHub.\n",
    "load_dotenv(override=False)\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"No se encontr√≥ GROQ_API_KEY.\\n\"\n",
    "        \"Crea un archivo .env con: GROQ_API_KEY=gsk_...\"\n",
    "    )\n",
    "\n",
    "# Configuraci√≥n centralizada: si hay que cambiar el modelo o los l√≠mites,\n",
    "# se toca aqu√≠ y no hay que buscar por todo el c√≥digo.\n",
    "MODEL       = \"llama-3.3-70b-versatile\"\n",
    "TEMPERATURE = 0.1\n",
    "MAX_RETRIES = 4\n",
    "INPUT_FILE  = \"../tickets.csv\"\n",
    "OUTPUT_FILE = \"../tickets_evaluated.csv\"\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "logger.info(f\"‚úÖ Cliente Groq inicializado. Modelo: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34092f6b",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering\n",
    "\n",
    "El **System Prompt** define el rol del modelo y el formato de salida esperado.\n",
    "El **User Prompt** aporta los datos concretos de cada fila.\n",
    "\n",
    "Forzamos la respuesta en JSON para poder parsearla de forma fiable sin depender\n",
    "de que el modelo use siempre el mismo formato de texto libre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6029edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert quality assurance analyst for a customer support team.\n",
    "Your task is to evaluate AI-generated replies to customer support tickets.\n",
    "\n",
    "You will receive a JSON object with two fields:\n",
    "- \"ticket\": the original customer message\n",
    "- \"reply\": the AI-generated response to evaluate\n",
    "\n",
    "Evaluate the reply on TWO dimensions using a scale from 1 to 5:\n",
    "\n",
    "CONTENT (relevance, correctness, completeness):\n",
    "  5 - Fully addresses all aspects of the ticket; accurate and complete\n",
    "  4 - Addresses the main issue; minor gaps or imprecisions\n",
    "  3 - Partially addresses the ticket; some relevant information missing\n",
    "  2 - Barely addresses the ticket; mostly off-topic or incorrect\n",
    "  1 - Does not address the ticket at all; irrelevant or harmful\n",
    "\n",
    "FORMAT (clarity, structure, grammar/spelling):\n",
    "  5 - Perfectly clear, well-structured, error-free, professional tone\n",
    "  4 - Clear and professional with minor formatting or grammar issues\n",
    "  3 - Understandable but with noticeable clarity or grammar problems\n",
    "  2 - Difficult to read; poor structure or significant grammar errors\n",
    "  1 - Incomprehensible; severely malformatted or full of errors\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Respond with ONLY a valid JSON object. Nothing else.\n",
    "- Do NOT use markdown code blocks.\n",
    "- The JSON must contain EXACTLY these four fields:\n",
    "\n",
    "{\n",
    "  \"content_score\": <integer between 1 and 5>,\n",
    "  \"content_explanation\": \"<one or two sentences>\",\n",
    "  \"format_score\": <integer between 1 and 5>,\n",
    "  \"format_explanation\": \"<one or two sentences>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_user_prompt(ticket: str, reply: str) -> str:\n",
    "    \"\"\"\n",
    "    Serializa ticket y reply como JSON para enviarlos al modelo.\n",
    "    Usar json.dumps() en lugar de f-strings garantiza el escape correcto\n",
    "    de comillas, saltos de l√≠nea y caracteres especiales.\n",
    "    \"\"\"\n",
    "    return json.dumps({\"ticket\": ticket, \"reply\": reply}, ensure_ascii=False)\n",
    "\n",
    "\n",
    "logger.info(\"System prompt y funci√≥n de user prompt definidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c0093",
   "metadata": {},
   "source": [
    "## 4. Llamada a la API con reintentos autom√°ticos\n",
    "\n",
    "Usamos `tenacity` para reintentar la llamada si la API falla por rate limit o\n",
    "por un error temporal. El backoff exponencial hace que espere cada vez m√°s\n",
    "entre reintentos, evitando saturar el servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    retry=retry_if_exception_type(Exception),\n",
    "    stop=stop_after_attempt(MAX_RETRIES),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    before_sleep=lambda retry_state: logger.warning(\n",
    "        f\"  ‚ö†Ô∏è Reintento {retry_state.attempt_number}/{MAX_RETRIES} \"\n",
    "        f\"tras error: {retry_state.outcome.exception()}\"\n",
    "    ),\n",
    ")\n",
    "def call_llm_api(ticket: str, reply: str) -> dict:\n",
    "    \"\"\"\n",
    "    Llama al LLM y devuelve un dict con los 4 campos de evaluaci√≥n.\n",
    "\n",
    "    Args:\n",
    "        ticket: Mensaje original del cliente.\n",
    "        reply:  Respuesta del sistema de IA a evaluar.\n",
    "\n",
    "    Returns:\n",
    "        Dict con content_score, content_explanation, format_score, format_explanation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la respuesta no cumple el schema esperado.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        temperature=TEMPERATURE,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": build_user_prompt(ticket, reply)},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    result = json.loads(response.choices[0].message.content.strip())\n",
    "\n",
    "    # Verificamos que el modelo devolvi√≥ todos los campos que necesitamos\n",
    "    required_fields = {\n",
    "        \"content_score\", \"content_explanation\",\n",
    "        \"format_score\",  \"format_explanation\",\n",
    "    }\n",
    "    missing = required_fields - result.keys()\n",
    "    if missing:\n",
    "        raise ValueError(f\"Campos faltantes en la respuesta: {missing}\")\n",
    "\n",
    "    # Verificamos que los scores son enteros en el rango correcto\n",
    "    for field in (\"content_score\", \"format_score\"):\n",
    "        score = result[field]\n",
    "        if not isinstance(score, int) or not (1 <= score <= 5):\n",
    "            raise ValueError(f\"Score inv√°lido en '{field}': {score!r}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "logger.info(\"Funci√≥n de llamada a API definida con retry autom√°tico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n de todos los tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tickets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Itera sobre el DataFrame y eval√∫a cada par (ticket, reply).\n",
    "    Si una fila falla, registra el error y contin√∫a con las siguientes\n",
    "    en lugar de detener todo el proceso.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame con columnas 'ticket' y 'reply'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame original con las 4 columnas de evaluaci√≥n a√±adidas.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        logger.info(f\"Evaluando fila {idx + 1}/{total}...\")\n",
    "\n",
    "        ticket = str(row.get(\"ticket\", \"\")).strip()\n",
    "        reply  = str(row.get(\"reply\",  \"\")).strip()\n",
    "\n",
    "        if not ticket or not reply:\n",
    "            logger.warning(f\"Fila {idx}: datos vac√≠os, se omite.\")\n",
    "            results.append({\n",
    "                \"content_score\": None, \"content_explanation\": \"Missing data\",\n",
    "                \"format_score\":  None, \"format_explanation\":  \"Missing data\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            evaluation = call_llm_api(ticket, reply)\n",
    "            results.append(evaluation)\n",
    "            logger.info(\n",
    "                f\"  ‚úì Content: {evaluation['content_score']}/5 | \"\n",
    "                f\"Format: {evaluation['format_score']}/5\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"  ‚úó Error permanente en fila {idx}: {e}\")\n",
    "            results.append({\n",
    "                \"content_score\": None, \"content_explanation\": f\"Error: {e}\",\n",
    "                \"format_score\":  None, \"format_explanation\":  f\"Error: {e}\",\n",
    "            })\n",
    "\n",
    "    return pd.concat([df.reset_index(drop=True), pd.DataFrame(results)], axis=1)\n",
    "\n",
    "\n",
    "logger.info(\"Funci√≥n de evaluaci√≥n definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lectura y validaci√≥n del CSV de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_csv(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee el CSV y comprueba que tiene las columnas necesarias antes de\n",
    "    hacer ninguna llamada a la API.\n",
    "\n",
    "    Args:\n",
    "        filepath: Ruta al archivo CSV.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame validado.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Si el archivo no existe.\n",
    "        ValueError: Si faltan columnas obligatorias.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥: {filepath}\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    logger.info(f\"CSV cargado: {len(df)} filas | columnas: {list(df.columns)}\")\n",
    "\n",
    "    missing_cols = {\"ticket\", \"reply\"} - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columnas faltantes en el CSV: {missing_cols}\")\n",
    "\n",
    "    empty_rows = df[[\"ticket\", \"reply\"]].isnull().any(axis=1).sum()\n",
    "    if empty_rows > 0:\n",
    "        logger.warning(f\"{empty_rows} filas con valores nulos ‚Äî se omitir√°n.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_input = load_and_validate_csv(INPUT_FILE)\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n y escritura del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluated = evaluate_tickets(df_input)\n",
    "\n",
    "output_columns = [\n",
    "    \"ticket\", \"reply\",\n",
    "    \"content_score\", \"content_explanation\",\n",
    "    \"format_score\",  \"format_explanation\",\n",
    "]\n",
    "df_output = df_evaluated[output_columns]\n",
    "\n",
    "df_output.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "logger.info(f\"‚úÖ Evaluaci√≥n completada. Resultado guardado en: {OUTPUT_FILE}\")\n",
    "\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"          RESUMEN DE EVALUACI√ìN\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Total de tickets procesados : {len(df_output)}\")\n",
    "print(f\"Content Score ‚Äî Media: {df_output['content_score'].mean():.2f} \"\n",
    "      f\"| Min: {df_output['content_score'].min()} \"\n",
    "      f\"| Max: {df_output['content_score'].max()}\")\n",
    "print(f\"Format Score  ‚Äî Media: {df_output['format_score'].mean():.2f} \"\n",
    "      f\"| Min: {df_output['format_score'].min()} \"\n",
    "      f\"| Max: {df_output['format_score'].max()}\")\n",
    "print(f\"Filas con error             : {df_output['content_score'].isnull().sum()}\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Escalabilidad: qu√© har√≠a con 1 mill√≥n de tickets\n",
    "\n",
    "### 1. Procesamiento concurrente con asyncio\n",
    "La versi√≥n actual procesa un ticket cada vez. Con `asyncio` y llamadas concurrentes\n",
    "controladas por un sem√°foro (`asyncio.Semaphore(50)`), el tiempo se reducir√≠a\n",
    "de d√≠as a horas.\n",
    "\n",
    "### 2. Arquitectura de cola con AWS SQS + Lambda\n",
    "Para escala real: **S3** recibe el CSV ‚Üí **Lambda** publica cada fila en **SQS** ‚Üí\n",
    "m√∫ltiples **Lambda workers** procesan en paralelo ‚Üí resultados en **DynamoDB**.\n",
    "Ventajas: escalado autom√°tico, tolerancia a fallos con Dead Letter Queue,\n",
    "coste pay-per-execution.\n",
    "\n",
    "### 3. Control de costes y observabilidad\n",
    "- Cach√© sem√°ntica (Redis) para tickets similares ya evaluados\n",
    "- Modelo m√°s barato para el grueso, modelo premium solo para scores 2-3\n",
    "- Alertas de presupuesto y logging de tokens consumidos por llamada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
